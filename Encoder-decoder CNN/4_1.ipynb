{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-fBGpkjC0qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from copy import deepcopy\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IlsqcU86bTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x_64 = []\n",
        "train_y_64 = []\n",
        "\n",
        "train_x_32 = []\n",
        "train_y_32 = []\n",
        "\n",
        "for i in range(1, 101, 1):\n",
        "    temperature = \"{:.2f}\".format(0.01 * i)\n",
        "    \n",
        "    file_path = '64_u_' + str(temperature) + '.txt'\n",
        "    u = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_v_' + str(temperature) + '.txt'\n",
        "    v = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_vel_' + str(temperature) + '.txt'\n",
        "    vel = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_pressure_' + str(temperature) + '.txt'\n",
        "    pressure = np.loadtxt(file_path)\n",
        "    \n",
        "    y_output = np.asarray([u,v,pressure])\n",
        "    x_input = np.asarray([u,v,pressure])\n",
        "    train_x_64.append(x_input)\n",
        "    train_y_64.append(y_output)\n",
        "    \n",
        "    file_path = '32_u_' + str(temperature) + '.txt'\n",
        "    u = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_v_' + str(temperature) + '.txt'\n",
        "    v = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_vel_' + str(temperature) + '.txt'\n",
        "    vel = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_pressure_' + str(temperature) + '.txt'\n",
        "    pressure = np.loadtxt(file_path)    \n",
        "    \n",
        "    y_output = np.asarray([u,v,pressure])\n",
        "    x_input = np.asarray([u,v,pressure])\n",
        "    \n",
        "    train_x_32.append(x_input)\n",
        "    train_y_32.append(y_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqLYDYwL7re1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_x_32 = []\n",
        "val_y_32 = []\n",
        "\n",
        "val_x_64 = []\n",
        "val_y_64 = []\n",
        "\n",
        "for i in range(1, 20, 2):\n",
        "    temperature = 0.05 * i + 0.005\n",
        "    temperature = \"{:.3f}\".format(temperature)\n",
        "    \n",
        "    file_path = '64_u_' + str(temperature) + '.txt'\n",
        "    u = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_v_' + str(temperature) + '.txt'\n",
        "    v = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_vel_' + str(temperature) + '.txt'\n",
        "    vel = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_pressure_' + str(temperature) + '.txt'\n",
        "    pressure = np.loadtxt(file_path)\n",
        "\n",
        "    x_input = np.asarray([u,v,pressure])\n",
        "    y_output = np.asarray([u,v,pressure])\n",
        "   \n",
        "    val_x_64.append(x_input)\n",
        "    val_y_64.append(y_output)\n",
        "    \n",
        "    file_path = '32_u_' + str(temperature) + '.txt'\n",
        "    u = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_v_' + str(temperature) + '.txt'\n",
        "    v = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_vel_' + str(temperature) + '.txt'\n",
        "    vel = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_pressure_' + str(temperature) + '.txt'\n",
        "    pressure = np.loadtxt(file_path)\n",
        "    \n",
        "    y_output = np.asarray([u,v,pressure])\n",
        "    x_input = np.asarray([u,v,pressure])\n",
        "    \n",
        "    val_x_32.append(x_input)\n",
        "    val_y_32.append(y_output)\n",
        "    \n",
        "    \n",
        "    temperature = 0.05 * i - 0.005\n",
        "    temperature = \"{:.3f}\".format(temperature)\n",
        "    \n",
        "    file_path = '64_u_' + str(temperature) + '.txt'\n",
        "    u = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_v_' + str(temperature) + '.txt'\n",
        "    v = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_vel_' + str(temperature) + '.txt'\n",
        "    vel = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '64_pressure_' + str(temperature) + '.txt'\n",
        "    pressure = np.loadtxt(file_path)\n",
        "    \n",
        "    x_input = np.asarray([u,v,pressure])\n",
        "    y_output = np.asarray([u,v,pressure])\n",
        "\n",
        "    val_x_64.append(x_input)\n",
        "    val_y_64.append(y_output)\n",
        "\n",
        "    file_path = '32_u_' + str(temperature) + '.txt'\n",
        "    u = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_v_' + str(temperature) + '.txt'\n",
        "    v = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_vel_' + str(temperature) + '.txt'\n",
        "    vel = np.loadtxt(file_path)\n",
        "    \n",
        "    file_path = '32_pressure_' + str(temperature) + '.txt'\n",
        "    pressure = np.loadtxt(file_path)\n",
        "    \n",
        "    x_input = np.asarray([u,v,pressure])\n",
        "    y_output = np.asarray([u,v,pressure])\n",
        "    \n",
        "    val_x_32.append(x_input)\n",
        "    val_y_32.append(y_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9XFaVGHDRWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, train_x, train_y):\n",
        "        self.x = train_x\n",
        "        self.y = train_y\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "      \n",
        "data_32 = dataset(train_x_32, train_y_32)\n",
        "data_64 = dataset(train_x_64, train_y_64)\n",
        "val_32 = dataset(val_x_32, val_y_32)\n",
        "val_64 = dataset(val_x_64, val_y_64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSjLRSP6DKdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show(image):\n",
        "    plt.imshow(image, vmin=np.amin(image), vmax=np.amax(image), cmap='hsv')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmj_IdYdD0pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "      \n",
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = input.view(1, 3, 24, 24)\n",
        "        return input\n",
        "\n",
        "class auto_encoder_high_res(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(auto_encoder_high_res, self).__init__()\n",
        "    \n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "        \n",
        "        nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "        \n",
        "        nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "        \n",
        "        nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "        \n",
        "        Flatten()\n",
        "    )\n",
        "    \n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(1728, 1728),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    \n",
        "    self.decoder = nn.Sequential(\n",
        "        \n",
        "        UnFlatten(),\n",
        "        \n",
        "        nn.ConvTranspose2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0, output_padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "        \n",
        "        nn.ConvTranspose2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0, output_padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "\n",
        "        nn.ConvTranspose2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0, output_padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "\n",
        "        nn.ConvTranspose2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0, output_padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "        \n",
        "        nn.ConvTranspose2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0, output_padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),     \n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "  \n",
        "  def downsample(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "  \n",
        "  def upsample(self, x):\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5jQbaQz04vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "      \n",
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = input.view(1, 3, 24, 24)\n",
        "        return input\n",
        "\n",
        "class auto_encoder_low_res(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(auto_encoder_low_res, self).__init__()\n",
        "    \n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3),\n",
        "\n",
        "        Flatten()\n",
        "    )\n",
        "    \n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(1728, 1728),\n",
        "        nn.Tanh()\n",
        "    )    \n",
        "    \n",
        "    self.decoder = nn.Sequential(\n",
        "        \n",
        "        UnFlatten(),\n",
        "        \n",
        "        nn.ConvTranspose2d(in_channels = 3, out_channels = 3, kernel_size = 9, stride = 1, padding = 0, output_padding = 0),\n",
        "        nn.Tanh(),\n",
        "        nn.BatchNorm2d(3)        \n",
        "    )\n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "  \n",
        "  def downsample(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "  \n",
        "  def upsample(self, x):\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2qINGKeKB-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fnn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(fnn, self).__init__()\n",
        "    self.encode = nn.Sequential(\n",
        "      nn.BatchNorm1d(1),  \n",
        "      nn.Linear(1728, 1728),\n",
        "      nn.BatchNorm1d(1),  \n",
        "      nn.Linear(1728,1728),  \n",
        "      nn.Tanh()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.encode(x)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWrXjxGQtgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "M = 0.01\n",
        "sdev = 0.001\n",
        "\n",
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        init.normal_(m.weight.data, mean=M, std=sdev)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight.data, mean=M, std=sdev)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm3d):\n",
        "        init.normal_(m.weight.data, mean=M, std=sdev)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.LSTMCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRU):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRUCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-HfCvHsQ12O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader_64 = torch.utils.data.DataLoader(data_64, batch_size = 1, shuffle = False)\n",
        "train_loader_32 = torch.utils.data.DataLoader(data_32, batch_size = 1, shuffle = False)\n",
        "test_loader_64 = torch.utils.data.DataLoader(val_64, batch_size = 1, shuffle = False)\n",
        "test_loader_32 = torch.utils.data.DataLoader(val_32, batch_size = 1, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mM39rvFBG6U",
        "colab_type": "code",
        "outputId": "ab7727ee-9204-49ab-b5e4-3d3ab639a887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "high_res = auto_encoder_high_res()\n",
        "high_res = high_res.double()\n",
        "high_res = high_res.cuda()\n",
        "low_res = auto_encoder_low_res()\n",
        "low_res = low_res.double()\n",
        "low_res = low_res.cuda()\n",
        "inter = fnn()\n",
        "inter = inter.double()\n",
        "inter = inter.cuda()\n",
        "path = \"weights_1.pth\"\n",
        "high_res.load_state_dict(torch.load(path))\n",
        "path = \"weights_2.pth\"\n",
        "low_res.load_state_dict(torch.load(path))\n",
        "'''\n",
        "path = \"weights_3.pth\"\n",
        "inter.load_state_dict(torch.load(path))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npath = \"weights_3.pth\"\\ninter.load_state_dict(torch.load(path))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op7gzb9nEPU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader_64 = torch.utils.data.DataLoader(data_64, batch_size = 1, shuffle = False)\n",
        "\n",
        "embed_64 = []\n",
        "\n",
        "for i, dat in enumerate((train_loader_64), 0):\n",
        "        inputs, labels = dat\n",
        "        inputs = inputs.resize_(1, 3, 64, 64)\n",
        "        labels = labels.resize_(1, 3, 64, 64)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = high_res.downsample(inputs)\n",
        "        embed_64.append(outputs[0].cpu().detach().numpy())\n",
        "        \n",
        "embed_64 = np.asarray(embed_64)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_5ogEPBOuvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader_32 = torch.utils.data.DataLoader(data_32, batch_size = 1, shuffle = False)\n",
        "\n",
        "embed_32 = []\n",
        "\n",
        "for i, dat in enumerate((train_loader_32), 0):\n",
        "        inputs, labels = dat\n",
        "        inputs = inputs.resize_(1, 3, 32, 32)\n",
        "        labels = labels.resize_(1, 3, 32, 32)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = low_res.downsample(inputs)\n",
        "        embed_32.append(outputs[0].cpu().detach().numpy())\n",
        "\n",
        "embed_32 = np.asarray(embed_32)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyd-36npIqiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_data = dataset(embed_32, embed_64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBfpBwtfYJW7",
        "colab_type": "code",
        "outputId": "34dd9514-bf71-40cb-82db-edb1b8618cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "inter.apply(weight_init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fnn(\n",
              "  (encode): Sequential(\n",
              "    (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): Linear(in_features=1728, out_features=1728, bias=True)\n",
              "    (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Linear(in_features=1728, out_features=1728, bias=True)\n",
              "    (4): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I8Djk1LL3gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.00005\n",
        "optimizer = optim.Adam(inter.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfJQgHYvLIv8",
        "colab_type": "code",
        "outputId": "d964f6bd-c3ad-4bed-94c4-26deb0f8ac4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13865
        }
      },
      "source": [
        "embed_loader = torch.utils.data.DataLoader(linear_data, batch_size = 1, shuffle = False)\n",
        "\n",
        "running_loss = 0.0\n",
        "num_epochs = 1001\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    validation_loss = 0.0\n",
        "    for i, dat in enumerate((embed_loader), 0):\n",
        "        inputs, labels = dat\n",
        "        inputs = inputs.resize_(1, 1, 1728)\n",
        "        labels = labels.resize_(1, 1, 1728)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = inter(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    running_loss/=100.0\n",
        "    print(\"Epoch: \",epoch+1,\"running_loss: \",running_loss*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1 running_loss:  0.005180668393352662\n",
            "Epoch:  2 running_loss:  0.004545217418878165\n",
            "Epoch:  3 running_loss:  0.004468010845710562\n",
            "Epoch:  4 running_loss:  0.0044340839729488405\n",
            "Epoch:  5 running_loss:  0.004099624440963735\n",
            "Epoch:  6 running_loss:  0.0038543513271238573\n",
            "Epoch:  7 running_loss:  0.003655729236117429\n",
            "Epoch:  8 running_loss:  0.003607296530229558\n",
            "Epoch:  9 running_loss:  0.003686741383134232\n",
            "Epoch:  10 running_loss:  0.0035595880386894633\n",
            "Epoch:  11 running_loss:  0.003404418256029867\n",
            "Epoch:  12 running_loss:  0.0033330099867721476\n",
            "Epoch:  13 running_loss:  0.00324163015560583\n",
            "Epoch:  14 running_loss:  0.003249202649354822\n",
            "Epoch:  15 running_loss:  0.0030072865608897098\n",
            "Epoch:  16 running_loss:  0.0029005614222850436\n",
            "Epoch:  17 running_loss:  0.0028564944121175395\n",
            "Epoch:  18 running_loss:  0.0028104072161416085\n",
            "Epoch:  19 running_loss:  0.002737899339805048\n",
            "Epoch:  20 running_loss:  0.0027113350486586943\n",
            "Epoch:  21 running_loss:  0.002677828651268299\n",
            "Epoch:  22 running_loss:  0.002638125335432025\n",
            "Epoch:  23 running_loss:  0.0026345664090476045\n",
            "Epoch:  24 running_loss:  0.0025510395199619005\n",
            "Epoch:  25 running_loss:  0.002542767318309218\n",
            "Epoch:  26 running_loss:  0.0025179175954590572\n",
            "Epoch:  27 running_loss:  0.0024620081001439365\n",
            "Epoch:  28 running_loss:  0.00241158327299961\n",
            "Epoch:  29 running_loss:  0.0024173434094286977\n",
            "Epoch:  30 running_loss:  0.002350153481187245\n",
            "Epoch:  31 running_loss:  0.0023751197623839023\n",
            "Epoch:  32 running_loss:  0.002283183035273001\n",
            "Epoch:  33 running_loss:  0.0022485307390394888\n",
            "Epoch:  34 running_loss:  0.0022598647734263532\n",
            "Epoch:  35 running_loss:  0.0021938068047742637\n",
            "Epoch:  36 running_loss:  0.002179338732952106\n",
            "Epoch:  37 running_loss:  0.0021589337253421913\n",
            "Epoch:  38 running_loss:  0.002133730333288661\n",
            "Epoch:  39 running_loss:  0.0020775332805246195\n",
            "Epoch:  40 running_loss:  0.002060067929537481\n",
            "Epoch:  41 running_loss:  0.0020425730724989736\n",
            "Epoch:  42 running_loss:  0.0020356141435724086\n",
            "Epoch:  43 running_loss:  0.001988583716768715\n",
            "Epoch:  44 running_loss:  0.001988522410368378\n",
            "Epoch:  45 running_loss:  0.001988509061226373\n",
            "Epoch:  46 running_loss:  0.0019646942326130403\n",
            "Epoch:  47 running_loss:  0.0019118854730867042\n",
            "Epoch:  48 running_loss:  0.0019255751136102695\n",
            "Epoch:  49 running_loss:  0.0018953386834015772\n",
            "Epoch:  50 running_loss:  0.0018830417014989497\n",
            "Epoch:  51 running_loss:  0.0018506855941468814\n",
            "Epoch:  52 running_loss:  0.0018328573745462016\n",
            "Epoch:  53 running_loss:  0.0018140144120464639\n",
            "Epoch:  54 running_loss:  0.0018304574974861746\n",
            "Epoch:  55 running_loss:  0.0018154330547529925\n",
            "Epoch:  56 running_loss:  0.00178614950353568\n",
            "Epoch:  57 running_loss:  0.0017744199002098758\n",
            "Epoch:  58 running_loss:  0.0017385950608003863\n",
            "Epoch:  59 running_loss:  0.00170674575648434\n",
            "Epoch:  60 running_loss:  0.0016999342976256808\n",
            "Epoch:  61 running_loss:  0.001660656988841398\n",
            "Epoch:  62 running_loss:  0.0016562211166249818\n",
            "Epoch:  63 running_loss:  0.0016441995243863994\n",
            "Epoch:  64 running_loss:  0.001631420062790116\n",
            "Epoch:  65 running_loss:  0.0016289592947591892\n",
            "Epoch:  66 running_loss:  0.001607311744444943\n",
            "Epoch:  67 running_loss:  0.001583776344308065\n",
            "Epoch:  68 running_loss:  0.001586392214607007\n",
            "Epoch:  69 running_loss:  0.0015487286368896763\n",
            "Epoch:  70 running_loss:  0.0015299609787175674\n",
            "Epoch:  71 running_loss:  0.0015231259149448197\n",
            "Epoch:  72 running_loss:  0.0014977431939069963\n",
            "Epoch:  73 running_loss:  0.0014896949983004728\n",
            "Epoch:  74 running_loss:  0.0014677082362984384\n",
            "Epoch:  75 running_loss:  0.0014836600738291773\n",
            "Epoch:  76 running_loss:  0.0014705556371820891\n",
            "Epoch:  77 running_loss:  0.0014301801580953245\n",
            "Epoch:  78 running_loss:  0.0014191008347967955\n",
            "Epoch:  79 running_loss:  0.0014304365076377213\n",
            "Epoch:  80 running_loss:  0.0013990341305791687\n",
            "Epoch:  81 running_loss:  0.0013930230919496345\n",
            "Epoch:  82 running_loss:  0.0013789514955816012\n",
            "Epoch:  83 running_loss:  0.0013753757928578556\n",
            "Epoch:  84 running_loss:  0.0013662585233383713\n",
            "Epoch:  85 running_loss:  0.0013431155465635572\n",
            "Epoch:  86 running_loss:  0.0013418620484403827\n",
            "Epoch:  87 running_loss:  0.0013414044935706364\n",
            "Epoch:  88 running_loss:  0.001308046783276139\n",
            "Epoch:  89 running_loss:  0.0013043928849171644\n",
            "Epoch:  90 running_loss:  0.0012899632254375298\n",
            "Epoch:  91 running_loss:  0.0012860768863613287\n",
            "Epoch:  92 running_loss:  0.001291092381867215\n",
            "Epoch:  93 running_loss:  0.001251717217148522\n",
            "Epoch:  94 running_loss:  0.0012595780096050584\n",
            "Epoch:  95 running_loss:  0.001248866456849083\n",
            "Epoch:  96 running_loss:  0.0012310011763631452\n",
            "Epoch:  97 running_loss:  0.0012273655287062802\n",
            "Epoch:  98 running_loss:  0.001207908349798482\n",
            "Epoch:  99 running_loss:  0.0012233082958177086\n",
            "Epoch:  100 running_loss:  0.0011967035858441995\n",
            "Epoch:  101 running_loss:  0.0011720089091921636\n",
            "Epoch:  102 running_loss:  0.0011845846900299408\n",
            "Epoch:  103 running_loss:  0.001173614383559921\n",
            "Epoch:  104 running_loss:  0.0011726021372011003\n",
            "Epoch:  105 running_loss:  0.0011382390671353596\n",
            "Epoch:  106 running_loss:  0.0011452580821352388\n",
            "Epoch:  107 running_loss:  0.0011572810452407027\n",
            "Epoch:  108 running_loss:  0.0011354746849478856\n",
            "Epoch:  109 running_loss:  0.0011096759072190747\n",
            "Epoch:  110 running_loss:  0.0011059174105052902\n",
            "Epoch:  111 running_loss:  0.001115647638192539\n",
            "Epoch:  112 running_loss:  0.0010905039285585931\n",
            "Epoch:  113 running_loss:  0.0010764680775312387\n",
            "Epoch:  114 running_loss:  0.0010682628179167518\n",
            "Epoch:  115 running_loss:  0.0010824955712344343\n",
            "Epoch:  116 running_loss:  0.001047360461014496\n",
            "Epoch:  117 running_loss:  0.0010363332909982823\n",
            "Epoch:  118 running_loss:  0.0010418573411316498\n",
            "Epoch:  119 running_loss:  0.0010515348775508861\n",
            "Epoch:  120 running_loss:  0.0010320334514799954\n",
            "Epoch:  121 running_loss:  0.0010133812933678155\n",
            "Epoch:  122 running_loss:  0.001009906578497049\n",
            "Epoch:  123 running_loss:  0.0010135035808657984\n",
            "Epoch:  124 running_loss:  0.0010092846266244208\n",
            "Epoch:  125 running_loss:  0.0009864653189026972\n",
            "Epoch:  126 running_loss:  0.0009817885229370534\n",
            "Epoch:  127 running_loss:  0.0009700335907548477\n",
            "Epoch:  128 running_loss:  0.0009924227428721338\n",
            "Epoch:  129 running_loss:  0.0009435159138276929\n",
            "Epoch:  130 running_loss:  0.000954888308394936\n",
            "Epoch:  131 running_loss:  0.0009276436560658098\n",
            "Epoch:  132 running_loss:  0.0009293211932034248\n",
            "Epoch:  133 running_loss:  0.0008791447788020184\n",
            "Epoch:  134 running_loss:  0.0008860244337679994\n",
            "Epoch:  135 running_loss:  0.0008906312545799833\n",
            "Epoch:  136 running_loss:  0.0008828061151365653\n",
            "Epoch:  137 running_loss:  0.0008315866496175734\n",
            "Epoch:  138 running_loss:  0.0008367790826388456\n",
            "Epoch:  139 running_loss:  0.000855942271071569\n",
            "Epoch:  140 running_loss:  0.0008471407506669363\n",
            "Epoch:  141 running_loss:  0.000814311367990596\n",
            "Epoch:  142 running_loss:  0.00080871401371115\n",
            "Epoch:  143 running_loss:  0.0008361439932249665\n",
            "Epoch:  144 running_loss:  0.0008165006309598102\n",
            "Epoch:  145 running_loss:  0.0007964478373069116\n",
            "Epoch:  146 running_loss:  0.000795629355910349\n",
            "Epoch:  147 running_loss:  0.0007950646454035052\n",
            "Epoch:  148 running_loss:  0.0007920068984091392\n",
            "Epoch:  149 running_loss:  0.0007710092423241209\n",
            "Epoch:  150 running_loss:  0.0007409501131821936\n",
            "Epoch:  151 running_loss:  0.000756686151776177\n",
            "Epoch:  152 running_loss:  0.0007486589437840175\n",
            "Epoch:  153 running_loss:  0.0007305316657411208\n",
            "Epoch:  154 running_loss:  0.0007255078686663352\n",
            "Epoch:  155 running_loss:  0.0007104563372888304\n",
            "Epoch:  156 running_loss:  0.0006990229372022228\n",
            "Epoch:  157 running_loss:  0.0006882173993921915\n",
            "Epoch:  158 running_loss:  0.0006724499046438141\n",
            "Epoch:  159 running_loss:  0.0006621104611381862\n",
            "Epoch:  160 running_loss:  0.0006802000676640183\n",
            "Epoch:  161 running_loss:  0.0006498088479290784\n",
            "Epoch:  162 running_loss:  0.0006715174373193316\n",
            "Epoch:  163 running_loss:  0.0006730759796778964\n",
            "Epoch:  164 running_loss:  0.0006664812871632876\n",
            "Epoch:  165 running_loss:  0.000628039942925083\n",
            "Epoch:  166 running_loss:  0.0006608624503725648\n",
            "Epoch:  167 running_loss:  0.0006325416025603056\n",
            "Epoch:  168 running_loss:  0.0006196388059165529\n",
            "Epoch:  169 running_loss:  0.0005966146300616248\n",
            "Epoch:  170 running_loss:  0.000593683290346459\n",
            "Epoch:  171 running_loss:  0.0005511535103362733\n",
            "Epoch:  172 running_loss:  0.0005789209584347016\n",
            "Epoch:  173 running_loss:  0.000539455104800996\n",
            "Epoch:  174 running_loss:  0.0005371774929834402\n",
            "Epoch:  175 running_loss:  0.0005320196950884652\n",
            "Epoch:  176 running_loss:  0.0005541982372587158\n",
            "Epoch:  177 running_loss:  0.0005421572717425348\n",
            "Epoch:  178 running_loss:  0.0005144728028057502\n",
            "Epoch:  179 running_loss:  0.0005279357372658768\n",
            "Epoch:  180 running_loss:  0.000532338870397511\n",
            "Epoch:  181 running_loss:  0.0004864980134662918\n",
            "Epoch:  182 running_loss:  0.0005186636849556364\n",
            "Epoch:  183 running_loss:  0.0004778549792780206\n",
            "Epoch:  184 running_loss:  0.0004732783479093518\n",
            "Epoch:  185 running_loss:  0.0004779233991007148\n",
            "Epoch:  186 running_loss:  0.00044062989587321394\n",
            "Epoch:  187 running_loss:  0.00048760220075159244\n",
            "Epoch:  188 running_loss:  0.0004687513908047332\n",
            "Epoch:  189 running_loss:  0.0004474129616563803\n",
            "Epoch:  190 running_loss:  0.00044199202186875344\n",
            "Epoch:  191 running_loss:  0.00047516038960022033\n",
            "Epoch:  192 running_loss:  0.0004429525953144805\n",
            "Epoch:  193 running_loss:  0.000449512903672879\n",
            "Epoch:  194 running_loss:  0.00044258596077456424\n",
            "Epoch:  195 running_loss:  0.0004169470716136018\n",
            "Epoch:  196 running_loss:  0.00040780650073639975\n",
            "Epoch:  197 running_loss:  0.0004087808844628091\n",
            "Epoch:  198 running_loss:  0.0003999330415882382\n",
            "Epoch:  199 running_loss:  0.0003991079092071977\n",
            "Epoch:  200 running_loss:  0.0003884701951237001\n",
            "Epoch:  201 running_loss:  0.000386623034138246\n",
            "Epoch:  202 running_loss:  0.0003941473441905581\n",
            "Epoch:  203 running_loss:  0.0003725449061382933\n",
            "Epoch:  204 running_loss:  0.0003965737025595415\n",
            "Epoch:  205 running_loss:  0.000371557714720318\n",
            "Epoch:  206 running_loss:  0.0003912786030410517\n",
            "Epoch:  207 running_loss:  0.00037463059084529753\n",
            "Epoch:  208 running_loss:  0.0003871990730243857\n",
            "Epoch:  209 running_loss:  0.00038289321140759145\n",
            "Epoch:  210 running_loss:  0.0003628208406739704\n",
            "Epoch:  211 running_loss:  0.0003709065218670252\n",
            "Epoch:  212 running_loss:  0.0003571414753031686\n",
            "Epoch:  213 running_loss:  0.0003511052065782711\n",
            "Epoch:  214 running_loss:  0.00035254245294020245\n",
            "Epoch:  215 running_loss:  0.0003408598508273768\n",
            "Epoch:  216 running_loss:  0.00033816194681776066\n",
            "Epoch:  217 running_loss:  0.00034512530505344626\n",
            "Epoch:  218 running_loss:  0.00033502028707176913\n",
            "Epoch:  219 running_loss:  0.0003381161175536176\n",
            "Epoch:  220 running_loss:  0.0003328256094320412\n",
            "Epoch:  221 running_loss:  0.00034721300079073984\n",
            "Epoch:  222 running_loss:  0.00034287529582055124\n",
            "Epoch:  223 running_loss:  0.0003366077203379976\n",
            "Epoch:  224 running_loss:  0.0003529296296989564\n",
            "Epoch:  225 running_loss:  0.00033928853289091237\n",
            "Epoch:  226 running_loss:  0.0003312224512424395\n",
            "Epoch:  227 running_loss:  0.00034200228392218903\n",
            "Epoch:  228 running_loss:  0.0003452683156830861\n",
            "Epoch:  229 running_loss:  0.0003322892045464582\n",
            "Epoch:  230 running_loss:  0.0003197372296291839\n",
            "Epoch:  231 running_loss:  0.00033942512312500593\n",
            "Epoch:  232 running_loss:  0.00033161334323001664\n",
            "Epoch:  233 running_loss:  0.00033477214824324754\n",
            "Epoch:  234 running_loss:  0.0003402912016695007\n",
            "Epoch:  235 running_loss:  0.0003300491458982176\n",
            "Epoch:  236 running_loss:  0.0003249512635717305\n",
            "Epoch:  237 running_loss:  0.0003384283572322272\n",
            "Epoch:  238 running_loss:  0.0003470777834507143\n",
            "Epoch:  239 running_loss:  0.00033123871043139153\n",
            "Epoch:  240 running_loss:  0.0003315218413358556\n",
            "Epoch:  241 running_loss:  0.0003349177052477639\n",
            "Epoch:  242 running_loss:  0.00033878660100220237\n",
            "Epoch:  243 running_loss:  0.0003415390562710911\n",
            "Epoch:  244 running_loss:  0.00032519959385686676\n",
            "Epoch:  245 running_loss:  0.000317857615989761\n",
            "Epoch:  246 running_loss:  0.00031989865539930063\n",
            "Epoch:  247 running_loss:  0.00033635920537328614\n",
            "Epoch:  248 running_loss:  0.00031660155579725676\n",
            "Epoch:  249 running_loss:  0.00030972288956508225\n",
            "Epoch:  250 running_loss:  0.0003160960991147514\n",
            "Epoch:  251 running_loss:  0.0003168228817943362\n",
            "Epoch:  252 running_loss:  0.00031667053145475533\n",
            "Epoch:  253 running_loss:  0.0003253907795961132\n",
            "Epoch:  254 running_loss:  0.00032361208374175927\n",
            "Epoch:  255 running_loss:  0.00030671467078740997\n",
            "Epoch:  256 running_loss:  0.00029010482541169886\n",
            "Epoch:  257 running_loss:  0.00030883879789976853\n",
            "Epoch:  258 running_loss:  0.000311062237964713\n",
            "Epoch:  259 running_loss:  0.0002976564077049922\n",
            "Epoch:  260 running_loss:  0.0002993140522054702\n",
            "Epoch:  261 running_loss:  0.0002964092277737204\n",
            "Epoch:  262 running_loss:  0.00029494289457247515\n",
            "Epoch:  263 running_loss:  0.0003053623714992169\n",
            "Epoch:  264 running_loss:  0.00032260196264681\n",
            "Epoch:  265 running_loss:  0.0003029800807128116\n",
            "Epoch:  266 running_loss:  0.000314143930899075\n",
            "Epoch:  267 running_loss:  0.00030913712510474657\n",
            "Epoch:  268 running_loss:  0.00030517996884350926\n",
            "Epoch:  269 running_loss:  0.0003183139004847843\n",
            "Epoch:  270 running_loss:  0.0003329699519460136\n",
            "Epoch:  271 running_loss:  0.0002937276536976128\n",
            "Epoch:  272 running_loss:  0.0002759456750818348\n",
            "Epoch:  273 running_loss:  0.0002987768095842315\n",
            "Epoch:  274 running_loss:  0.0002920372779236927\n",
            "Epoch:  275 running_loss:  0.00030253084682235455\n",
            "Epoch:  276 running_loss:  0.00030026097914296533\n",
            "Epoch:  277 running_loss:  0.00027997310957611975\n",
            "Epoch:  278 running_loss:  0.00031412345006777827\n",
            "Epoch:  279 running_loss:  0.0003134230983005456\n",
            "Epoch:  280 running_loss:  0.0003162877732586539\n",
            "Epoch:  281 running_loss:  0.00028370657426550293\n",
            "Epoch:  282 running_loss:  0.00034135732876235765\n",
            "Epoch:  283 running_loss:  0.00032651191132698483\n",
            "Epoch:  284 running_loss:  0.0002920272114254411\n",
            "Epoch:  285 running_loss:  0.000287446643054259\n",
            "Epoch:  286 running_loss:  0.00031277030960245124\n",
            "Epoch:  287 running_loss:  0.0002740089271316139\n",
            "Epoch:  288 running_loss:  0.0002777686484475223\n",
            "Epoch:  289 running_loss:  0.00028099006380243806\n",
            "Epoch:  290 running_loss:  0.00031576183674966404\n",
            "Epoch:  291 running_loss:  0.0002938790714782169\n",
            "Epoch:  292 running_loss:  0.00028662203275363324\n",
            "Epoch:  293 running_loss:  0.00028848202004154786\n",
            "Epoch:  294 running_loss:  0.00028318873944093136\n",
            "Epoch:  295 running_loss:  0.00028928177884581587\n",
            "Epoch:  296 running_loss:  0.0002948351723828997\n",
            "Epoch:  297 running_loss:  0.00027703992715485265\n",
            "Epoch:  298 running_loss:  0.00028663698904096403\n",
            "Epoch:  299 running_loss:  0.00028541695009511877\n",
            "Epoch:  300 running_loss:  0.0002940896026924247\n",
            "Epoch:  301 running_loss:  0.000249555806130879\n",
            "Epoch:  302 running_loss:  0.0002831558353229752\n",
            "Epoch:  303 running_loss:  0.0002680700027503821\n",
            "Epoch:  304 running_loss:  0.0002901031186020324\n",
            "Epoch:  305 running_loss:  0.00028229510724843963\n",
            "Epoch:  306 running_loss:  0.00028546341726649394\n",
            "Epoch:  307 running_loss:  0.0003021053470930536\n",
            "Epoch:  308 running_loss:  0.00025640950070525366\n",
            "Epoch:  309 running_loss:  0.0002579194599261547\n",
            "Epoch:  310 running_loss:  0.0003651634107522789\n",
            "Epoch:  311 running_loss:  0.0002808118954333492\n",
            "Epoch:  312 running_loss:  0.00025919276725946077\n",
            "Epoch:  313 running_loss:  0.00025876266991389297\n",
            "Epoch:  314 running_loss:  0.0002826021558944248\n",
            "Epoch:  315 running_loss:  0.00027119748899560495\n",
            "Epoch:  316 running_loss:  0.00033246463758143956\n",
            "Epoch:  317 running_loss:  0.0003197944097167765\n",
            "Epoch:  318 running_loss:  0.0003535029464354423\n",
            "Epoch:  319 running_loss:  0.00033416115766399517\n",
            "Epoch:  320 running_loss:  0.0003819342809262211\n",
            "Epoch:  321 running_loss:  0.00024788430888457317\n",
            "Epoch:  322 running_loss:  0.00027746875828356767\n",
            "Epoch:  323 running_loss:  0.000278027786976672\n",
            "Epoch:  324 running_loss:  0.0002741077235530267\n",
            "Epoch:  325 running_loss:  0.0002524924033608907\n",
            "Epoch:  326 running_loss:  0.00028522589713204157\n",
            "Epoch:  327 running_loss:  0.0002544459011166723\n",
            "Epoch:  328 running_loss:  0.0002929815636138394\n",
            "Epoch:  329 running_loss:  0.0002561905263897268\n",
            "Epoch:  330 running_loss:  0.00028161595133394807\n",
            "Epoch:  331 running_loss:  0.00025507562441035627\n",
            "Epoch:  332 running_loss:  0.0002785282529703398\n",
            "Epoch:  333 running_loss:  0.0002636935456600498\n",
            "Epoch:  334 running_loss:  0.0003417973222506459\n",
            "Epoch:  335 running_loss:  0.00024889307677183057\n",
            "Epoch:  336 running_loss:  0.0002672792606449031\n",
            "Epoch:  337 running_loss:  0.0003044609333889531\n",
            "Epoch:  338 running_loss:  0.0002825358943194111\n",
            "Epoch:  339 running_loss:  0.00023816065145638285\n",
            "Epoch:  340 running_loss:  0.00023486328978696992\n",
            "Epoch:  341 running_loss:  0.00023964922393347098\n",
            "Epoch:  342 running_loss:  0.00025735999894790245\n",
            "Epoch:  343 running_loss:  0.00024380655712548003\n",
            "Epoch:  344 running_loss:  0.0002967416779319915\n",
            "Epoch:  345 running_loss:  0.0002771925879335496\n",
            "Epoch:  346 running_loss:  0.0002743198706711499\n",
            "Epoch:  347 running_loss:  0.0002506361145598754\n",
            "Epoch:  348 running_loss:  0.0002987422770405019\n",
            "Epoch:  349 running_loss:  0.0002595085375354121\n",
            "Epoch:  350 running_loss:  0.00036178842753825706\n",
            "Epoch:  351 running_loss:  0.0005096992442710317\n",
            "Epoch:  352 running_loss:  0.00045903277656209843\n",
            "Epoch:  353 running_loss:  0.00034739393146381004\n",
            "Epoch:  354 running_loss:  0.0002417909974320871\n",
            "Epoch:  355 running_loss:  0.00021220274268111505\n",
            "Epoch:  356 running_loss:  0.0002161510929066169\n",
            "Epoch:  357 running_loss:  0.00021893935856274305\n",
            "Epoch:  358 running_loss:  0.00023792846855199906\n",
            "Epoch:  359 running_loss:  0.00024710418254243993\n",
            "Epoch:  360 running_loss:  0.00022255979984213889\n",
            "Epoch:  361 running_loss:  0.00031486969056853065\n",
            "Epoch:  362 running_loss:  0.00028671771678147665\n",
            "Epoch:  363 running_loss:  0.00032644054363679573\n",
            "Epoch:  364 running_loss:  0.00031690120851591576\n",
            "Epoch:  365 running_loss:  0.00037989201170852434\n",
            "Epoch:  366 running_loss:  0.0003655913641928982\n",
            "Epoch:  367 running_loss:  0.00030656059961345323\n",
            "Epoch:  368 running_loss:  0.00030056918565241135\n",
            "Epoch:  369 running_loss:  0.0003148121842592127\n",
            "Epoch:  370 running_loss:  0.00030103359624049315\n",
            "Epoch:  371 running_loss:  0.00026914346489639667\n",
            "Epoch:  372 running_loss:  0.00024007977530969972\n",
            "Epoch:  373 running_loss:  0.00022690831876457813\n",
            "Epoch:  374 running_loss:  0.0002339164594404059\n",
            "Epoch:  375 running_loss:  0.0002178592616167378\n",
            "Epoch:  376 running_loss:  0.00024824035835488856\n",
            "Epoch:  377 running_loss:  0.00021506746909959445\n",
            "Epoch:  378 running_loss:  0.00022438754256004815\n",
            "Epoch:  379 running_loss:  0.0002495746598964774\n",
            "Epoch:  380 running_loss:  0.0002516602598422874\n",
            "Epoch:  381 running_loss:  0.0002906813438807753\n",
            "Epoch:  382 running_loss:  0.0002944285330661811\n",
            "Epoch:  383 running_loss:  0.00035930796341147316\n",
            "Epoch:  384 running_loss:  0.0005300601966914841\n",
            "Epoch:  385 running_loss:  0.00037941115103734964\n",
            "Epoch:  386 running_loss:  0.00024171616660447948\n",
            "Epoch:  387 running_loss:  0.0002367824662225306\n",
            "Epoch:  388 running_loss:  0.00022738935400395998\n",
            "Epoch:  389 running_loss:  0.00020545016862092688\n",
            "Epoch:  390 running_loss:  0.00022830025378780261\n",
            "Epoch:  391 running_loss:  0.00020558259326976832\n",
            "Epoch:  392 running_loss:  0.00021668876482058315\n",
            "Epoch:  393 running_loss:  0.00024398244079719658\n",
            "Epoch:  394 running_loss:  0.0002479624093494687\n",
            "Epoch:  395 running_loss:  0.0003037395748310837\n",
            "Epoch:  396 running_loss:  0.0002322833159559984\n",
            "Epoch:  397 running_loss:  0.0002997943382355117\n",
            "Epoch:  398 running_loss:  0.00028372925093756953\n",
            "Epoch:  399 running_loss:  0.000301279103403559\n",
            "Epoch:  400 running_loss:  0.000419351129327823\n",
            "Epoch:  401 running_loss:  0.0006153927408817225\n",
            "Epoch:  402 running_loss:  0.0004109098052206234\n",
            "Epoch:  403 running_loss:  0.0002178597083166452\n",
            "Epoch:  404 running_loss:  0.00021564036612601746\n",
            "Epoch:  405 running_loss:  0.0001997952006859456\n",
            "Epoch:  406 running_loss:  0.00023670205133937997\n",
            "Epoch:  407 running_loss:  0.0002119874100759525\n",
            "Epoch:  408 running_loss:  0.00023951349186882587\n",
            "Epoch:  409 running_loss:  0.0002859443666557117\n",
            "Epoch:  410 running_loss:  0.00025285603091078286\n",
            "Epoch:  411 running_loss:  0.0003089699637597708\n",
            "Epoch:  412 running_loss:  0.0003050743463270315\n",
            "Epoch:  413 running_loss:  0.0003491104333541996\n",
            "Epoch:  414 running_loss:  0.00037548939969468204\n",
            "Epoch:  415 running_loss:  0.0003206644663014855\n",
            "Epoch:  416 running_loss:  0.0002774438394194207\n",
            "Epoch:  417 running_loss:  0.0002572364795439545\n",
            "Epoch:  418 running_loss:  0.00023076883662736058\n",
            "Epoch:  419 running_loss:  0.0002047693937038899\n",
            "Epoch:  420 running_loss:  0.0002387900639295229\n",
            "Epoch:  421 running_loss:  0.00025667744851120547\n",
            "Epoch:  422 running_loss:  0.00023504856494362217\n",
            "Epoch:  423 running_loss:  0.00027889626508698973\n",
            "Epoch:  424 running_loss:  0.0002887844104373995\n",
            "Epoch:  425 running_loss:  0.00038788009889669564\n",
            "Epoch:  426 running_loss:  0.0004626081786527228\n",
            "Epoch:  427 running_loss:  0.0005320028908818303\n",
            "Epoch:  428 running_loss:  0.0004100957423167143\n",
            "Epoch:  429 running_loss:  0.00022965362734354798\n",
            "Epoch:  430 running_loss:  0.0001910765569888912\n",
            "Epoch:  431 running_loss:  0.00018953983816180384\n",
            "Epoch:  432 running_loss:  0.0002383670593118527\n",
            "Epoch:  433 running_loss:  0.0002194328434963036\n",
            "Epoch:  434 running_loss:  0.00026700134070647085\n",
            "Epoch:  435 running_loss:  0.00030511625288943186\n",
            "Epoch:  436 running_loss:  0.0002919212779381335\n",
            "Epoch:  437 running_loss:  0.00031818483376227607\n",
            "Epoch:  438 running_loss:  0.0003375054971716972\n",
            "Epoch:  439 running_loss:  0.0003437679932077467\n",
            "Epoch:  440 running_loss:  0.00034981158290835364\n",
            "Epoch:  441 running_loss:  0.00030818187731374485\n",
            "Epoch:  442 running_loss:  0.0002621259241558158\n",
            "Epoch:  443 running_loss:  0.00028270573957120203\n",
            "Epoch:  444 running_loss:  0.0002559787517198022\n",
            "Epoch:  445 running_loss:  0.0001999534819943517\n",
            "Epoch:  446 running_loss:  0.00020630530272640292\n",
            "Epoch:  447 running_loss:  0.00024392603746622402\n",
            "Epoch:  448 running_loss:  0.00022948592512428778\n",
            "Epoch:  449 running_loss:  0.00023777844858591647\n",
            "Epoch:  450 running_loss:  0.00021413862087801025\n",
            "Epoch:  451 running_loss:  0.0002917545282833893\n",
            "Epoch:  452 running_loss:  0.00026891137764075475\n",
            "Epoch:  453 running_loss:  0.0003475634188454506\n",
            "Epoch:  454 running_loss:  0.0003367133755617138\n",
            "Epoch:  455 running_loss:  0.00029096259456000434\n",
            "Epoch:  456 running_loss:  0.0003524128195178236\n",
            "Epoch:  457 running_loss:  0.0004198176361816445\n",
            "Epoch:  458 running_loss:  0.0002837071132893436\n",
            "Epoch:  459 running_loss:  0.00026388218040619655\n",
            "Epoch:  460 running_loss:  0.00023859619345459925\n",
            "Epoch:  461 running_loss:  0.00023353380571429283\n",
            "Epoch:  462 running_loss:  0.00024906379083347146\n",
            "Epoch:  463 running_loss:  0.0002764326712262594\n",
            "Epoch:  464 running_loss:  0.00024015477030738704\n",
            "Epoch:  465 running_loss:  0.0002291823070992143\n",
            "Epoch:  466 running_loss:  0.0002647576525751349\n",
            "Epoch:  467 running_loss:  0.0002804680999721916\n",
            "Epoch:  468 running_loss:  0.0002821969347983986\n",
            "Epoch:  469 running_loss:  0.0003026231685268662\n",
            "Epoch:  470 running_loss:  0.00034883328104531327\n",
            "Epoch:  471 running_loss:  0.0003507951911766566\n",
            "Epoch:  472 running_loss:  0.00029819883462623494\n",
            "Epoch:  473 running_loss:  0.00025207071617427165\n",
            "Epoch:  474 running_loss:  0.00028373467916950726\n",
            "Epoch:  475 running_loss:  0.0003515949873002427\n",
            "Epoch:  476 running_loss:  0.0003636193647429236\n",
            "Epoch:  477 running_loss:  0.0004704029908274347\n",
            "Epoch:  478 running_loss:  0.0003460801174572165\n",
            "Epoch:  479 running_loss:  0.00023828656666176145\n",
            "Epoch:  480 running_loss:  0.00022345000433178947\n",
            "Epoch:  481 running_loss:  0.00020670103689358047\n",
            "Epoch:  482 running_loss:  0.0002174398364193107\n",
            "Epoch:  483 running_loss:  0.0002205666280554824\n",
            "Epoch:  484 running_loss:  0.00025876710430182827\n",
            "Epoch:  485 running_loss:  0.00038955915380056906\n",
            "Epoch:  486 running_loss:  0.0003344267472366531\n",
            "Epoch:  487 running_loss:  0.0003488900274116395\n",
            "Epoch:  488 running_loss:  0.0002792771955941939\n",
            "Epoch:  489 running_loss:  0.00029040123525038105\n",
            "Epoch:  490 running_loss:  0.0003536159079865\n",
            "Epoch:  491 running_loss:  0.0002625656697824488\n",
            "Epoch:  492 running_loss:  0.0003046140296646638\n",
            "Epoch:  493 running_loss:  0.00031657395125508713\n",
            "Epoch:  494 running_loss:  0.00025963098566579357\n",
            "Epoch:  495 running_loss:  0.00027495855353973977\n",
            "Epoch:  496 running_loss:  0.000334462529618793\n",
            "Epoch:  497 running_loss:  0.0002209027620315312\n",
            "Epoch:  498 running_loss:  0.000281747395874056\n",
            "Epoch:  499 running_loss:  0.00024211747434577671\n",
            "Epoch:  500 running_loss:  0.0002652977404463795\n",
            "Epoch:  501 running_loss:  0.00022628217640410843\n",
            "Epoch:  502 running_loss:  0.000291671265730889\n",
            "Epoch:  503 running_loss:  0.00023549187353663113\n",
            "Epoch:  504 running_loss:  0.00026722590054819033\n",
            "Epoch:  505 running_loss:  0.00035627326034281295\n",
            "Epoch:  506 running_loss:  0.000603315412066295\n",
            "Epoch:  507 running_loss:  0.0004144934025475725\n",
            "Epoch:  508 running_loss:  0.0002333440444201663\n",
            "Epoch:  509 running_loss:  0.00020771549167162214\n",
            "Epoch:  510 running_loss:  0.00020314043089420843\n",
            "Epoch:  511 running_loss:  0.00019874658503239346\n",
            "Epoch:  512 running_loss:  0.0004163201263924811\n",
            "Epoch:  513 running_loss:  0.0005880863043013013\n",
            "Epoch:  514 running_loss:  0.00034606497624242483\n",
            "Epoch:  515 running_loss:  0.0002158940719299965\n",
            "Epoch:  516 running_loss:  0.0002129551240739184\n",
            "Epoch:  517 running_loss:  0.00020099404988698898\n",
            "Epoch:  518 running_loss:  0.00025604486360335764\n",
            "Epoch:  519 running_loss:  0.000254927929673217\n",
            "Epoch:  520 running_loss:  0.0002774715124259013\n",
            "Epoch:  521 running_loss:  0.00034937639587071054\n",
            "Epoch:  522 running_loss:  0.0002980638075047986\n",
            "Epoch:  523 running_loss:  0.0003475176733854807\n",
            "Epoch:  524 running_loss:  0.0003418733075824954\n",
            "Epoch:  525 running_loss:  0.00035959339356859277\n",
            "Epoch:  526 running_loss:  0.0002246247428324424\n",
            "Epoch:  527 running_loss:  0.0003177428883878635\n",
            "Epoch:  528 running_loss:  0.00021496346447274917\n",
            "Epoch:  529 running_loss:  0.00024978452293161757\n",
            "Epoch:  530 running_loss:  0.0002427547441829692\n",
            "Epoch:  531 running_loss:  0.0002512608236838944\n",
            "Epoch:  532 running_loss:  0.00023479854750788187\n",
            "Epoch:  533 running_loss:  0.00023876375214224689\n",
            "Epoch:  534 running_loss:  0.00023448100547908998\n",
            "Epoch:  535 running_loss:  0.00026907427135769164\n",
            "Epoch:  536 running_loss:  0.0002179135330983992\n",
            "Epoch:  537 running_loss:  0.00026101558082488056\n",
            "Epoch:  538 running_loss:  0.00023860270285706503\n",
            "Epoch:  539 running_loss:  0.00031028252781524723\n",
            "Epoch:  540 running_loss:  0.00030285927556279507\n",
            "Epoch:  541 running_loss:  0.00029613847344121705\n",
            "Epoch:  542 running_loss:  0.00027202518830768315\n",
            "Epoch:  543 running_loss:  0.00033001378322089345\n",
            "Epoch:  544 running_loss:  0.00027087573424556795\n",
            "Epoch:  545 running_loss:  0.00036784919170299924\n",
            "Epoch:  546 running_loss:  0.0003264819976245922\n",
            "Epoch:  547 running_loss:  0.0004320384429812664\n",
            "Epoch:  548 running_loss:  0.000425648104191116\n",
            "Epoch:  549 running_loss:  0.00019018559613364345\n",
            "Epoch:  550 running_loss:  0.00016397229965590338\n",
            "Epoch:  551 running_loss:  0.00018723949325236578\n",
            "Epoch:  552 running_loss:  0.00019864928680393256\n",
            "Epoch:  553 running_loss:  0.00022202928722203917\n",
            "Epoch:  554 running_loss:  0.0002523866777186596\n",
            "Epoch:  555 running_loss:  0.00028267920755595363\n",
            "Epoch:  556 running_loss:  0.00029843096734028315\n",
            "Epoch:  557 running_loss:  0.0003474207622068169\n",
            "Epoch:  558 running_loss:  0.0003599665051556548\n",
            "Epoch:  559 running_loss:  0.00044345801443033966\n",
            "Epoch:  560 running_loss:  0.0005402184655517\n",
            "Epoch:  561 running_loss:  0.0003802886140235641\n",
            "Epoch:  562 running_loss:  0.00020833975687243976\n",
            "Epoch:  563 running_loss:  0.00021760497446461418\n",
            "Epoch:  564 running_loss:  0.00019125117183350272\n",
            "Epoch:  565 running_loss:  0.00018698949038104416\n",
            "Epoch:  566 running_loss:  0.0002121652785231275\n",
            "Epoch:  567 running_loss:  0.00021912809057709868\n",
            "Epoch:  568 running_loss:  0.00024176753480715268\n",
            "Epoch:  569 running_loss:  0.00025221710130830306\n",
            "Epoch:  570 running_loss:  0.0003385067691506844\n",
            "Epoch:  571 running_loss:  0.0007439511928540784\n",
            "Epoch:  572 running_loss:  0.0005682086708454508\n",
            "Epoch:  573 running_loss:  0.0007479093038252732\n",
            "Epoch:  574 running_loss:  0.000398664396659829\n",
            "Epoch:  575 running_loss:  0.00031816928811362663\n",
            "Epoch:  576 running_loss:  0.0002457355121674187\n",
            "Epoch:  577 running_loss:  0.00025500555565204785\n",
            "Epoch:  578 running_loss:  0.000253208241653737\n",
            "Epoch:  579 running_loss:  0.00022500692388704546\n",
            "Epoch:  580 running_loss:  0.00026600650114991714\n",
            "Epoch:  581 running_loss:  0.0002689607465662924\n",
            "Epoch:  582 running_loss:  0.00030185275565180504\n",
            "Epoch:  583 running_loss:  0.0003204544395399857\n",
            "Epoch:  584 running_loss:  0.00034329472067373267\n",
            "Epoch:  585 running_loss:  0.00034695829860362964\n",
            "Epoch:  586 running_loss:  0.0003160143049136596\n",
            "Epoch:  587 running_loss:  0.0002996980152918385\n",
            "Epoch:  588 running_loss:  0.00026331157902312433\n",
            "Epoch:  589 running_loss:  0.0002562459510000699\n",
            "Epoch:  590 running_loss:  0.00023878475234217483\n",
            "Epoch:  591 running_loss:  0.00023926025667845083\n",
            "Epoch:  592 running_loss:  0.00024412608996357918\n",
            "Epoch:  593 running_loss:  0.00029240051793147804\n",
            "Epoch:  594 running_loss:  0.0001985604247018279\n",
            "Epoch:  595 running_loss:  0.0002668521660338586\n",
            "Epoch:  596 running_loss:  0.000273808124353429\n",
            "Epoch:  597 running_loss:  0.000247680728031343\n",
            "Epoch:  598 running_loss:  0.00027648112971008086\n",
            "Epoch:  599 running_loss:  0.00023794529596065836\n",
            "Epoch:  600 running_loss:  0.0003020287389192248\n",
            "Epoch:  601 running_loss:  0.00035580545677495407\n",
            "Epoch:  602 running_loss:  0.0004255223031409055\n",
            "Epoch:  603 running_loss:  0.0002889885790065113\n",
            "Epoch:  604 running_loss:  0.0005611837624791346\n",
            "Epoch:  605 running_loss:  0.0006055682614872791\n",
            "Epoch:  606 running_loss:  0.00044745131937229724\n",
            "Epoch:  607 running_loss:  0.00016964584948066022\n",
            "Epoch:  608 running_loss:  0.0002009266017851959\n",
            "Epoch:  609 running_loss:  0.0001857313740119943\n",
            "Epoch:  610 running_loss:  0.00021833027905107414\n",
            "Epoch:  611 running_loss:  0.00021783144052626006\n",
            "Epoch:  612 running_loss:  0.00023543168030325487\n",
            "Epoch:  613 running_loss:  0.00030705614027768063\n",
            "Epoch:  614 running_loss:  0.0002816016096631918\n",
            "Epoch:  615 running_loss:  0.0003338680818562213\n",
            "Epoch:  616 running_loss:  0.00031600359256780106\n",
            "Epoch:  617 running_loss:  0.00030166900820241055\n",
            "Epoch:  618 running_loss:  0.0003054754581454632\n",
            "Epoch:  619 running_loss:  0.00022862790156483024\n",
            "Epoch:  620 running_loss:  0.00023241994497470107\n",
            "Epoch:  621 running_loss:  0.0002729980143849248\n",
            "Epoch:  622 running_loss:  0.00025054102736797015\n",
            "Epoch:  623 running_loss:  0.0002938663585599122\n",
            "Epoch:  624 running_loss:  0.00028298535138345226\n",
            "Epoch:  625 running_loss:  0.00033308414643316923\n",
            "Epoch:  626 running_loss:  0.00031437011616267086\n",
            "Epoch:  627 running_loss:  0.00030134302472387104\n",
            "Epoch:  628 running_loss:  0.0008589971261016615\n",
            "Epoch:  629 running_loss:  0.0004042215162783244\n",
            "Epoch:  630 running_loss:  0.00024164213607405039\n",
            "Epoch:  631 running_loss:  0.0003621740082655725\n",
            "Epoch:  632 running_loss:  0.0002907876540025784\n",
            "Epoch:  633 running_loss:  0.0003039275003657386\n",
            "Epoch:  634 running_loss:  0.000266408046608758\n",
            "Epoch:  635 running_loss:  0.0003134443361877652\n",
            "Epoch:  636 running_loss:  0.00029414549399139797\n",
            "Epoch:  637 running_loss:  0.0002949285515025074\n",
            "Epoch:  638 running_loss:  0.0002932660228676632\n",
            "Epoch:  639 running_loss:  0.0002514041641752458\n",
            "Epoch:  640 running_loss:  0.0003596178815132295\n",
            "Epoch:  641 running_loss:  0.0004485134428713546\n",
            "Epoch:  642 running_loss:  0.00037194279725450314\n",
            "Epoch:  643 running_loss:  0.00027125237715660796\n",
            "Epoch:  644 running_loss:  0.00027224634812272994\n",
            "Epoch:  645 running_loss:  0.000212812496068097\n",
            "Epoch:  646 running_loss:  0.00022429997592146152\n",
            "Epoch:  647 running_loss:  0.0002249870268880515\n",
            "Epoch:  648 running_loss:  0.0002456564067259433\n",
            "Epoch:  649 running_loss:  0.00022959446868879493\n",
            "Epoch:  650 running_loss:  0.0002662592256232786\n",
            "Epoch:  651 running_loss:  0.00025729278811704244\n",
            "Epoch:  652 running_loss:  0.0002899946088236577\n",
            "Epoch:  653 running_loss:  0.0002747893206696889\n",
            "Epoch:  654 running_loss:  0.0003334959042149776\n",
            "Epoch:  655 running_loss:  0.00033555153095721663\n",
            "Epoch:  656 running_loss:  0.0004002741253501232\n",
            "Epoch:  657 running_loss:  0.0004733359773548962\n",
            "Epoch:  658 running_loss:  0.00038267233577096585\n",
            "Epoch:  659 running_loss:  0.00020508326244577936\n",
            "Epoch:  660 running_loss:  0.00020480866746153497\n",
            "Epoch:  661 running_loss:  0.00018163458972407422\n",
            "Epoch:  662 running_loss:  0.0001927886797070275\n",
            "Epoch:  663 running_loss:  0.00020797230749989952\n",
            "Epoch:  664 running_loss:  0.00023255305622858457\n",
            "Epoch:  665 running_loss:  0.000261348102056555\n",
            "Epoch:  666 running_loss:  0.00034182965767283566\n",
            "Epoch:  667 running_loss:  0.00027586608792830737\n",
            "Epoch:  668 running_loss:  0.0003058935880214322\n",
            "Epoch:  669 running_loss:  0.0002780569364185526\n",
            "Epoch:  670 running_loss:  0.00033683725815324904\n",
            "Epoch:  671 running_loss:  0.00024649552320400335\n",
            "Epoch:  672 running_loss:  0.00032586010590722025\n",
            "Epoch:  673 running_loss:  0.0003450269736826432\n",
            "Epoch:  674 running_loss:  0.0005008313389588306\n",
            "Epoch:  675 running_loss:  0.0002017887516164497\n",
            "Epoch:  676 running_loss:  0.0003407947022475385\n",
            "Epoch:  677 running_loss:  0.0002343297246362669\n",
            "Epoch:  678 running_loss:  0.00020931776143151196\n",
            "Epoch:  679 running_loss:  0.00025869785931441894\n",
            "Epoch:  680 running_loss:  0.00022676731572839046\n",
            "Epoch:  681 running_loss:  0.00026604006195439\n",
            "Epoch:  682 running_loss:  0.00027615266270450596\n",
            "Epoch:  683 running_loss:  0.0002984015270308034\n",
            "Epoch:  684 running_loss:  0.0003414891394735248\n",
            "Epoch:  685 running_loss:  0.00026526574527941526\n",
            "Epoch:  686 running_loss:  0.0002534813906918618\n",
            "Epoch:  687 running_loss:  0.00029338265784935877\n",
            "Epoch:  688 running_loss:  0.0002839325165242549\n",
            "Epoch:  689 running_loss:  0.00028729361502675315\n",
            "Epoch:  690 running_loss:  0.0002353282648170489\n",
            "Epoch:  691 running_loss:  0.0002464456151432679\n",
            "Epoch:  692 running_loss:  0.00033010104570620766\n",
            "Epoch:  693 running_loss:  0.0002907852826041379\n",
            "Epoch:  694 running_loss:  0.0002957323664836482\n",
            "Epoch:  695 running_loss:  0.00030580220357445844\n",
            "Epoch:  696 running_loss:  0.0002851079196634438\n",
            "Epoch:  697 running_loss:  0.0003047242037027822\n",
            "Epoch:  698 running_loss:  0.0007883721715665595\n",
            "Epoch:  699 running_loss:  0.0005715409877253858\n",
            "Epoch:  700 running_loss:  0.0005674329787730894\n",
            "Epoch:  701 running_loss:  0.00033906665887605046\n",
            "Epoch:  702 running_loss:  0.00022570779764649967\n",
            "Epoch:  703 running_loss:  0.00022767700162608964\n",
            "Epoch:  704 running_loss:  0.0002558855828714691\n",
            "Epoch:  705 running_loss:  0.0002762624862745276\n",
            "Epoch:  706 running_loss:  0.0002696348973282283\n",
            "Epoch:  707 running_loss:  0.0003351211314230462\n",
            "Epoch:  708 running_loss:  0.0003342021923279566\n",
            "Epoch:  709 running_loss:  0.00037726730499132975\n",
            "Epoch:  710 running_loss:  0.0003131758923420147\n",
            "Epoch:  711 running_loss:  0.0003027639027348009\n",
            "Epoch:  712 running_loss:  0.0003548654806716509\n",
            "Epoch:  713 running_loss:  0.00024683830474318147\n",
            "Epoch:  714 running_loss:  0.00022333416283988668\n",
            "Epoch:  715 running_loss:  0.00022262951410234242\n",
            "Epoch:  716 running_loss:  0.0002430031948302943\n",
            "Epoch:  717 running_loss:  0.00021440048837238944\n",
            "Epoch:  718 running_loss:  0.0002544634364938328\n",
            "Epoch:  719 running_loss:  0.00022733031990297188\n",
            "Epoch:  720 running_loss:  0.00024224633293720067\n",
            "Epoch:  721 running_loss:  0.0002608789746908237\n",
            "Epoch:  722 running_loss:  0.00027487337507985647\n",
            "Epoch:  723 running_loss:  0.00020701635377843214\n",
            "Epoch:  724 running_loss:  0.0002713880767312186\n",
            "Epoch:  725 running_loss:  0.0003185446340196369\n",
            "Epoch:  726 running_loss:  0.00028513597371900034\n",
            "Epoch:  727 running_loss:  0.00022503871624122559\n",
            "Epoch:  728 running_loss:  0.0008005510374165736\n",
            "Epoch:  729 running_loss:  0.0003260219339193099\n",
            "Epoch:  730 running_loss:  0.00026009714638822873\n",
            "Epoch:  731 running_loss:  0.0002928526578923317\n",
            "Epoch:  732 running_loss:  0.0003483563597870281\n",
            "Epoch:  733 running_loss:  0.00034332967775769634\n",
            "Epoch:  734 running_loss:  0.00032179505165798105\n",
            "Epoch:  735 running_loss:  0.0002714402177722605\n",
            "Epoch:  736 running_loss:  0.0003096174772565802\n",
            "Epoch:  737 running_loss:  0.0003002144534374833\n",
            "Epoch:  738 running_loss:  0.00023988681690057111\n",
            "Epoch:  739 running_loss:  0.00024187328678242344\n",
            "Epoch:  740 running_loss:  0.00024428045250662385\n",
            "Epoch:  741 running_loss:  0.0002118179458763577\n",
            "Epoch:  742 running_loss:  0.000231035393181398\n",
            "Epoch:  743 running_loss:  0.00024867291251299824\n",
            "Epoch:  744 running_loss:  0.0002463384370769413\n",
            "Epoch:  745 running_loss:  0.0002757680208789156\n",
            "Epoch:  746 running_loss:  0.00022527257699982454\n",
            "Epoch:  747 running_loss:  0.0002996371735984178\n",
            "Epoch:  748 running_loss:  0.0002619490196359647\n",
            "Epoch:  749 running_loss:  0.0003226908086776742\n",
            "Epoch:  750 running_loss:  0.00027244053268934314\n",
            "Epoch:  751 running_loss:  0.00027139800422352984\n",
            "Epoch:  752 running_loss:  0.00026300613765898944\n",
            "Epoch:  753 running_loss:  0.00027008609851328927\n",
            "Epoch:  754 running_loss:  0.0003005070818122794\n",
            "Epoch:  755 running_loss:  0.0004528848494978406\n",
            "Epoch:  756 running_loss:  0.00042533039221713607\n",
            "Epoch:  757 running_loss:  0.00029302035400914124\n",
            "Epoch:  758 running_loss:  0.0003328572276946353\n",
            "Epoch:  759 running_loss:  0.00026955160555879287\n",
            "Epoch:  760 running_loss:  0.0003196330948906341\n",
            "Epoch:  761 running_loss:  0.0002493085042603891\n",
            "Epoch:  762 running_loss:  0.00022782858743807625\n",
            "Epoch:  763 running_loss:  0.00022538884468555726\n",
            "Epoch:  764 running_loss:  0.00022551098578021129\n",
            "Epoch:  765 running_loss:  0.00021261451086516437\n",
            "Epoch:  766 running_loss:  0.00023129958944654617\n",
            "Epoch:  767 running_loss:  0.00021914514460733703\n",
            "Epoch:  768 running_loss:  0.0002701628306892189\n",
            "Epoch:  769 running_loss:  0.0002572119046942472\n",
            "Epoch:  770 running_loss:  0.0002959940070480678\n",
            "Epoch:  771 running_loss:  0.0003710122863254498\n",
            "Epoch:  772 running_loss:  0.000561255311240153\n",
            "Epoch:  773 running_loss:  0.00040773677476867675\n",
            "Epoch:  774 running_loss:  0.00023686661091195096\n",
            "Epoch:  775 running_loss:  0.0002147642327985051\n",
            "Epoch:  776 running_loss:  0.00023810413339404613\n",
            "Epoch:  777 running_loss:  0.0003067974787853804\n",
            "Epoch:  778 running_loss:  0.00022140795793446557\n",
            "Epoch:  779 running_loss:  0.0002633909692027443\n",
            "Epoch:  780 running_loss:  0.00029742021903180475\n",
            "Epoch:  781 running_loss:  0.00023229713505160387\n",
            "Epoch:  782 running_loss:  0.00027895125664735614\n",
            "Epoch:  783 running_loss:  0.0002275894258886338\n",
            "Epoch:  784 running_loss:  0.0002600104203775234\n",
            "Epoch:  785 running_loss:  0.0002350976066419405\n",
            "Epoch:  786 running_loss:  0.0002905420326804804\n",
            "Epoch:  787 running_loss:  0.00035797773508198746\n",
            "Epoch:  788 running_loss:  0.0003702734512760496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-52208ee9eddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlFmjvtcdZ0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(inter.state_dict(), \"weights_3.pth\")\n",
        "files.download(\"weights_3.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIRmci3RS9AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_embed_64 = []\n",
        "\n",
        "for i, dat in enumerate((embed_loader), 0):\n",
        "        inputs, labels = dat\n",
        "        inputs = inputs.resize_(1, 1, 1728)\n",
        "        labels = labels.resize_(1, 1, 1728)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        \n",
        "        outputs = inter(inputs)\n",
        "        output_embed_64.append(outputs[0][0].cpu().detach().numpy())\n",
        "\n",
        "output_embed_64 = np.asarray(output_embed_64)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwvjHujTTgWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = dataset(embed_64, embed_64)\n",
        "\n",
        "final_loader = torch.utils.data.DataLoader(final_data, batch_size = 1, shuffle = False)\n",
        "\n",
        "final_output = []\n",
        "\n",
        "for i, dat in enumerate((final_loader), 0):\n",
        "        inputs, labels = dat\n",
        "        inputs = inputs.resize_(1, 1, 1728)\n",
        "        labels = labels.resize_(1, 1, 1728)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        \n",
        "        outputs = high_res.upsample(inputs)\n",
        "        \n",
        "        final_output.append(outputs[0].cpu().detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acx_p2iaUMUI",
        "colab_type": "code",
        "outputId": "0a816198-7abd-4cf9-82ca-b9c248f61f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "show(final_output[50][0])\n",
        "show(train_y_64[50][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB+PffkF1vOHoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBXE71dql4gNqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnJhyZTrdZoW",
        "colab_type": "code",
        "outputId": "aa63ed29-95fe-430f-fb97-884e1db5a785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "L2_loss = 0.0\n",
        "\n",
        "for i in range(100):\n",
        "  L2_loss += np.linalg.norm(train_x_64[i][0]-final_output[i][0])\n",
        "\n",
        "print(L2_loss)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26.962595506731297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK4DXsqjeTox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}